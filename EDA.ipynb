{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d19805a",
   "metadata": {},
   "source": [
    "# Set up packages and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e0d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ba08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "transactions = spark.read.options(    \n",
    "        header=True,  \n",
    "        inferSchema=True\n",
    "    ).csv(\n",
    "        os.path.join(DATA_FOLDER, 'sales_train.csv'), \n",
    "    )\n",
    "\n",
    "items = spark.read.options(    \n",
    "        header=True,  \n",
    "        inferSchema=True\n",
    "    ).csv(\n",
    "        os.path.join(DATA_FOLDER, 'items.csv'), \n",
    "    )\n",
    "\n",
    "item_categories = spark.read.options(    \n",
    "        header=True,  \n",
    "        inferSchema=True\n",
    "    ).csv(\n",
    "        os.path.join(DATA_FOLDER, 'item_categories.csv'), \n",
    "    )\n",
    "\n",
    "shops = spark.read.options(    \n",
    "        header=True,  \n",
    "        inferSchema=True\n",
    "    ).csv(\n",
    "        os.path.join(DATA_FOLDER, 'shops.csv'), \n",
    "    )\n",
    "\n",
    "test = spark.read.options(    \n",
    "        header=True,  \n",
    "        inferSchema=True\n",
    "    ).csv(\n",
    "        os.path.join(DATA_FOLDER, 'test.csv'), \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa264c1e",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf45bb",
   "metadata": {},
   "source": [
    "## Look at dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc5a71",
   "metadata": {},
   "source": [
    "Print the top 10 rows and the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b1e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2935849\n",
      "+----------+--------------+-------+-------+----------+------------+\n",
      "|      date|date_block_num|shop_id|item_id|item_price|item_cnt_day|\n",
      "+----------+--------------+-------+-------+----------+------------+\n",
      "|02.01.2013|             0|     59|  22154|     999.0|         1.0|\n",
      "|03.01.2013|             0|     25|   2552|     899.0|         1.0|\n",
      "|05.01.2013|             0|     25|   2552|     899.0|        -1.0|\n",
      "|06.01.2013|             0|     25|   2554|   1709.05|         1.0|\n",
      "|15.01.2013|             0|     25|   2555|    1099.0|         1.0|\n",
      "|10.01.2013|             0|     25|   2564|     349.0|         1.0|\n",
      "|02.01.2013|             0|     25|   2565|     549.0|         1.0|\n",
      "|04.01.2013|             0|     25|   2572|     239.0|         1.0|\n",
      "|11.01.2013|             0|     25|   2572|     299.0|         1.0|\n",
      "|03.01.2013|             0|     25|   2573|     299.0|         3.0|\n",
      "+----------+--------------+-------+-------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total number of rows: 22170\n",
      "+--------------------+-------+----------------+\n",
      "|           item_name|item_id|item_category_id|\n",
      "+--------------------+-------+----------------+\n",
      "|! ВО ВЛАСТИ НАВАЖ...|      0|              40|\n",
      "|!ABBYY FineReader...|      1|              76|\n",
      "|***В ЛУЧАХ СЛАВЫ ...|      2|              40|\n",
      "|***ГОЛУБАЯ ВОЛНА ...|      3|              40|\n",
      "|***КОРОБКА (СТЕКЛ...|      4|              40|\n",
      "|***НОВЫЕ АМЕРИКАН...|      5|              40|\n",
      "|***УДАР ПО ВОРОТА...|      6|              40|\n",
      "|***УДАР ПО ВОРОТА...|      7|              40|\n",
      "|***ЧАЙ С МУССОЛИН...|      8|              40|\n",
      "|***ШУГАРЛЭНДСКИЙ ...|      9|              40|\n",
      "+--------------------+-------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total number of rows: 84\n",
      "+--------------------+----------------+\n",
      "|  item_category_name|item_category_id|\n",
      "+--------------------+----------------+\n",
      "|PC - Гарнитуры/На...|               0|\n",
      "|    Аксессуары - PS2|               1|\n",
      "|    Аксессуары - PS3|               2|\n",
      "|    Аксессуары - PS4|               3|\n",
      "|    Аксессуары - PSP|               4|\n",
      "| Аксессуары - PSVita|               5|\n",
      "|Аксессуары - XBOX...|               6|\n",
      "|Аксессуары - XBOX...|               7|\n",
      "|      Билеты (Цифра)|               8|\n",
      "|     Доставка товара|               9|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total number of rows: 60\n",
      "+--------------------+-------+\n",
      "|           shop_name|shop_id|\n",
      "+--------------------+-------+\n",
      "|!Якутск Орджоники...|      0|\n",
      "|\"!Якутск ТЦ \"\"Цен...|      1|\n",
      "|\"Адыгея ТЦ \"\"Мега\"\"\"|      2|\n",
      "|\"Балашиха ТРК \"\"О...|      3|\n",
      "|\"Волжский ТЦ \"\"Во...|      4|\n",
      "|\"Вологда ТРЦ \"\"Ма...|      5|\n",
      "|Воронеж (Плеханов...|      6|\n",
      "|\"Воронеж ТРЦ \"\"Ма...|      7|\n",
      "|\"Воронеж ТРЦ Сити...|      8|\n",
      "|   Выездная Торговля|      9|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total number of rows: 214200\n",
      "+---+-------+-------+\n",
      "| ID|shop_id|item_id|\n",
      "+---+-------+-------+\n",
      "|  0|      5|   5037|\n",
      "|  1|      5|   5320|\n",
      "|  2|      5|   5233|\n",
      "|  3|      5|   5232|\n",
      "|  4|      5|   5268|\n",
      "|  5|      5|   5039|\n",
      "|  6|      5|   5041|\n",
      "|  7|      5|   5046|\n",
      "|  8|      5|   5319|\n",
      "|  9|      5|   5003|\n",
      "+---+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total number of rows: {}'.format(transactions.count()))\n",
    "transactions.show(10)\n",
    "\n",
    "print('Total number of rows: {}'.format(items.count()))\n",
    "items.show(10)\n",
    "\n",
    "print('Total number of rows: {}'.format(item_categories.count()))\n",
    "item_categories.show(10)\n",
    "\n",
    "print('Total number of rows: {}'.format(shops.count()))\n",
    "shops.show(10)\n",
    "\n",
    "print('Total number of rows: {}'.format(test.count()))\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54e709",
   "metadata": {},
   "source": [
    "We see that in the training dataset, there are 2935849 transactions, 22170 items, 84 item categories and 60 shops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2a9d9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722865db",
   "metadata": {},
   "source": [
    "## Join all data onto the transactions dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd270e",
   "metadata": {},
   "source": [
    "Upon inspection, one sees that all features appearing in the training dataframes above can be joined onto the transactions dataframe using the appropriate ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317e4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView('transactions')\n",
    "items.createOrReplaceTempView('items')\n",
    "\n",
    "transactions = spark.sql(('SELECT transactions.*, items.item_name, items.item_category_id '\n",
    "                  ' FROM transactions '\n",
    "                  ' LEFT JOIN items '\n",
    "                  '  ON transactions.item_id = items.item_id '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c10627",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView('transactions')\n",
    "item_categories.createOrReplaceTempView('item_categories')\n",
    "\n",
    "transactions = spark.sql(('SELECT transactions.*, item_categories.item_category_name '\n",
    "                  ' FROM transactions '\n",
    "                  ' LEFT JOIN item_categories '\n",
    "                  '  ON transactions.item_category_id = item_categories.item_category_id '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4ad880",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView('transactions')\n",
    "shops.createOrReplaceTempView('shops')\n",
    "\n",
    "transactions = spark.sql(('SELECT transactions.*, shops.shop_name '\n",
    "                  ' FROM transactions '\n",
    "                  ' LEFT JOIN shops '\n",
    "                  '  ON transactions.shop_id = shops.shop_id '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059eeb29",
   "metadata": {},
   "source": [
    "## Extract the day, month and year from the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb043fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.withColumn('date', F.to_date(transactions.date, format='dd.MM.yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c28aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.withColumn('day', F.dayofyear(transactions.date))\n",
    "transactions = transactions.withColumn('month', F.month(transactions.date))\n",
    "transactions = transactions.withColumn('year', F.year(transactions.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9516b5",
   "metadata": {},
   "source": [
    "## Extract text-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582bb9e",
   "metadata": {},
   "source": [
    "Define a stemmer that can handle both Russian and English text using nltk's Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0026b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stemmer = SnowballStemmer('english')\n",
    "ru_stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695e6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Removes punctuation from string, unwanted unicode characters, and numbers. Returns in lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "    \n",
    "    Returns:\n",
    "        The cleaned text after filtered by the regex expression and made lowercase.\n",
    "    \n",
    "    For more information on the unicode categories used in the regex expression see here:\n",
    "    https://www.regular-expressions.info/unicode.html#category\n",
    "    \n",
    "    >>> clean_text(\"!$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ Can't, - Trademark™ ...「（Punctuation）」42.32 ?\")\n",
    "    cant trademark punctuation\n",
    "    \n",
    "    \"\"\"\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove apostrophes \n",
    "    text = text.replace(\"'\", \"\")\n",
    "    \n",
    "    # Define regex unicode Categories and strip from string\n",
    "    remove = regex.compile('[\\p{C}|\\p{M}|\\p{P}|\\p{S}|\\p{Z}|\\p{N}]+', regex.UNICODE)\n",
    "    text = remove.sub(\" \", text).strip()\n",
    "    \n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def stemmer(text):\n",
    "    \"\"\"Identify the words written in Cyrillic and Latin characters in a string,\n",
    "    and apply a Russian or English stemmer, respectively.\n",
    "    \n",
    "    Args:\n",
    "        text(str): The string whose Cyrillic and Latin text will be stemmed.\n",
    "    \n",
    "    Returns:\n",
    "        A stemmed version of the text.\n",
    "    \"\"\"\n",
    "    cyr_regex = regex.compile('\\p{Cyrillic}+', regex.UNICODE)\n",
    "    lat_regex = regex.compile('\\p{Latin}+', regex.UNICODE)\n",
    "    \n",
    "    text = clean_text(text)\n",
    "\n",
    "    words = re.split('\\s', text)\n",
    "    stemmed_text = ''\n",
    "    for word in words:\n",
    "        ru = regex.search(cyr_regex, word)\n",
    "        en = regex.search(lat_regex, word)\n",
    "        if ru:\n",
    "            stemmed_word = ru_stemmer.stem(word)\n",
    "        elif en:\n",
    "            stemmed_word = en_stemmer.stem(word)\n",
    "        else:\n",
    "            stemmed_word = word\n",
    "        stemmed_text = stemmed_text + ' ' + stemmed_word\n",
    "    # remove leading space from stemmed_text\n",
    "    stemmed_text = stemmed_text[1:]\n",
    "    \n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e976a8",
   "metadata": {},
   "source": [
    "Demonstrate function on sample text from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5056fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кин blu ray\n"
     ]
    }
   ],
   "source": [
    "text = '(Кино) - Blu-Ray'\n",
    "\n",
    "print(stemmer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997625e",
   "metadata": {},
   "source": [
    "Apply stemmer to columns containing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb6c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_stemmer = F.udf(stemmer, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8bdbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.withColumn('stemmed_item_name', udf_stemmer(transactions.item_name))\n",
    "transactions = transactions.withColumn('stemmed_item_category_name', udf_stemmer(transactions.item_category_name))\n",
    "transactions = transactions.withColumn('stemmed_shop_name', udf_stemmer(transactions.shop_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569e808",
   "metadata": {},
   "source": [
    "## Display the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d7c9c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 447, in read_udfs\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\cloudpickle\\cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'regex'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b31838a1c804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 447, in read_udfs\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\cloudpickle\\cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'regex'\n"
     ]
    }
   ],
   "source": [
    "transactions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a31d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
