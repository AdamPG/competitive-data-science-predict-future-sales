{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f83359",
   "metadata": {},
   "source": [
    "# Import packages and load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d912a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "from itertools import product\n",
    "import gc\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852ef838",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "sales = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\n",
    "shops = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n",
    "items = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\n",
    "item_cats = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c2592",
   "metadata": {},
   "source": [
    "# Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26728747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc135515",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc0f01",
   "metadata": {},
   "source": [
    "## Create the base training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bdcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "all_data = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a4663",
   "metadata": {},
   "source": [
    "## Aggregate sales and revenue by month, item id and shop id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb89e30",
   "metadata": {},
   "source": [
    "Add item revenue as a feature to the transactions dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec30e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['item_revenue'] = sales.item_cnt_day * sales.item_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab6fcd",
   "metadata": {},
   "source": [
    "Aggregate sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb7b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "# Fix column names\n",
    "gb.columns = ['target' if col == 'item_cnt_day' else col for col in gb.columns.values]\n",
    "# Join it to the grid\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb.columns = ['target_shop' if col == 'item_cnt_day' else col for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb.columns = ['target_item' if col == 'item_cnt_day' else col for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "# del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97752e10",
   "metadata": {},
   "source": [
    "Aggregate revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45007a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_revenue':'sum'})\n",
    "# Fix column names\n",
    "gb.columns = ['revenue' if col == 'item_revenue' else col for col in gb.columns.values]\n",
    "# Join it to the grid\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_revenue':'sum'})\n",
    "gb.columns = ['revenue_shop' if col == 'item_revenue' else col for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_revenue':'sum'})\n",
    "gb.columns = ['revenue_item' if col == 'item_revenue' else col for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "# del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f2d7a",
   "metadata": {},
   "source": [
    "## Normalize numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36eec58",
   "metadata": {},
   "source": [
    "We choose to normalize using a min-max scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec10f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of numerical columns to normalize\n",
    "num_cols = [col for col in all_data.columns if col.startswith('target') or col.startswith('revenue')]\n",
    "\n",
    "# Normalize the columns\n",
    "for col in num_cols:\n",
    "    col_min = all_data[col].min()\n",
    "    col_max = all_data[col].max()\n",
    "    all_data[col + '_normalized'] = (all_data[col] - col_min) / (col_max - col_min)\n",
    "    \n",
    "# Drop unnormalized columns except for 'target'\n",
    "cols_to_drop = [col for col in num_cols if col != 'target']\n",
    "all_data.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c684a4",
   "metadata": {},
   "source": [
    "## Add values from previous months as features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edf635",
   "metadata": {},
   "source": [
    "Create new features using lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9be62b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f74557d9804bd9a42a4efa857f7b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols + ['target']))\n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12354a",
   "metadata": {},
   "source": [
    "Normalized targets and revenues without a lag are not available as features in the test dataset, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d0136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['target_normalized', 'revenue_normalized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0744a",
   "metadata": {},
   "source": [
    "## Mean encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c85259",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "cat_cols = ['shop_id', 'item_id', 'item_category_id']\n",
    "\n",
    "for col in cat_cols:\n",
    "    all_data[col + '_enc'] = np.nan\n",
    "\n",
    "    for train_index, val_index in kf.split(all_data):\n",
    "        train_data, val_data = all_data.iloc[train_index].copy(), all_data.iloc[val_index].copy()\n",
    "        target_mean = train_data.groupby(col).target.mean()\n",
    "        val_data[col + '_enc'] = val_data[col].map(target_mean)\n",
    "        all_data.iloc[val_index] = val_data\n",
    "\n",
    "    all_data[col + '_enc'].fillna(0.3343, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549e253",
   "metadata": {},
   "source": [
    "## Extract text-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e97b0",
   "metadata": {},
   "source": [
    "### Stem the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27571ef",
   "metadata": {},
   "source": [
    "Define a stemmer that can handle both Russian and English text using nltk's Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eabdba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "ru_stemmer = SnowballStemmer('russian', ignore_stopwords=True)\n",
    "\n",
    "cyr_regex = regex.compile('\\p{Cyrillic}+', regex.UNICODE)\n",
    "lat_regex = regex.compile('\\p{Latin}+', regex.UNICODE)\n",
    "\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "ru_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b693b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Removes punctuation from string, unwanted unicode characters, and numbers. Returns in lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "    \n",
    "    Returns:\n",
    "        The cleaned text after filtered by the regex expression and made lowercase.\n",
    "    \n",
    "    For more information on the unicode categories used in the regex expression see here:\n",
    "    https://www.regular-expressions.info/unicode.html#category\n",
    "    \n",
    "    >>> clean_text(\"!$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ Can't, - Trademark™ ...「（Punctuation）」42.32 ?\")\n",
    "    cant trademark punctuation\n",
    "    \n",
    "    \"\"\"\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove apostrophes \n",
    "    text = text.replace(\"'\", \"\")\n",
    "    \n",
    "    # Define regex unicode Categories and strip from string\n",
    "    remove = regex.compile('[\\p{C}|\\p{M}|\\p{P}|\\p{S}|\\p{Z}|\\p{N}]+', regex.UNICODE)\n",
    "    text = remove.sub(\" \", text).strip()\n",
    "    \n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3472a",
   "metadata": {},
   "source": [
    "Apply clean_text to stopwords to make removal easier in the preprocess_text function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aca83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_en_stopwords = [clean_text(word) for word in en_stopwords]\n",
    "cleaned_ru_stopwords = [clean_text(word) for word in ru_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d448eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Identify the words written in Cyrillic and Latin characters in a string,\n",
    "    remove stop words and apply a Russian or English stemmer, respectively.\n",
    "    \n",
    "    Args:\n",
    "        text(str): The string whose Cyrillic and Latin text will be stemmed.\n",
    "    \n",
    "    Returns:\n",
    "        A stemmed version of the text.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    text = clean_text(text)\n",
    "\n",
    "    words = re.split('\\s', text)\n",
    "    stemmed_sentence = ''\n",
    "    for word in words:\n",
    "        ru = regex.search(cyr_regex, word)\n",
    "        en = regex.search(lat_regex, word)\n",
    "        if ru:\n",
    "            # drop stopwords from the sentence\n",
    "            if word in cleaned_ru_stopwords:\n",
    "                continue\n",
    "            stemmed_word = ru_stemmer.stem(word)\n",
    "        elif en:\n",
    "            # drop stopwords from the sentence\n",
    "            if word in cleaned_en_stopwords:\n",
    "                continue\n",
    "            stemmed_word = en_stemmer.stem(word)\n",
    "        else:\n",
    "            stemmed_word = word\n",
    "        stemmed_sentence = stemmed_sentence + ' ' + stemmed_word\n",
    "    \n",
    "    return stemmed_sentence[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a7643",
   "metadata": {},
   "source": [
    "Demonstrate function on sample text from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc735c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кин blu ray'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '(Кино) - Blu-Ray'\n",
    "\n",
    "preprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3148d2",
   "metadata": {},
   "source": [
    "Apply stemmer to columns containing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7101056",
   "metadata": {},
   "outputs": [],
   "source": [
    "shops['preprocessed_shop_name'] = shops['shop_name'].map(preprocess_text)\n",
    "items['preprocessed_item_name'] = items['item_name'].map(preprocess_text)\n",
    "item_cats['preprocessed_item_category_name'] = item_cats['item_category_name'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c229c",
   "metadata": {},
   "source": [
    "### Vectorize using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98f245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45db070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_shop_names = vectorizer.fit_transform(shops['preprocessed_shop_name'].values)\n",
    "tfidf_shop_names = pd.DataFrame.sparse.from_spmatrix(tfidf_shop_names)\n",
    "tfidf_shop_names.columns = ['vect_shop_name_' + str(col) for col in tfidf_shop_names.columns]\n",
    "shops = shops.join(tfidf_shop_names)\n",
    "\n",
    "tfidf_item_names = vectorizer.fit_transform(items['preprocessed_item_name'].values)\n",
    "tfidf_item_names = pd.DataFrame.sparse.from_spmatrix(tfidf_item_names)\n",
    "tfidf_item_names.columns = ['vect_item_name_' + str(col) for col in tfidf_item_names.columns]\n",
    "items = items.join(tfidf_item_names)\n",
    "\n",
    "tfidf_item_cat_names = vectorizer.fit_transform(item_cats['preprocessed_item_category_name'].values)\n",
    "tfidf_item_cat_names = pd.DataFrame.sparse.from_spmatrix(tfidf_item_cat_names)\n",
    "tfidf_item_cat_names.columns = ['vect_item_category_name_' + str(col) for col in tfidf_item_cat_names.columns]\n",
    "item_cats = item_cats.join(tfidf_item_cat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cac91",
   "metadata": {},
   "source": [
    "### Join TFIDF-encoded item/item category/shop names onto the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5409b628",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8dbd8b8ec8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvect_item_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvect_item_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvect_item_cat_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_cats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   8108\u001b[0m         \u001b[0;36m5\u001b[0m  \u001b[0mK5\u001b[0m  \u001b[0mA5\u001b[0m  \u001b[0mNaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8109\u001b[0m         \"\"\"\n\u001b[0;32m-> 8110\u001b[0;31m         return self._join_compat(\n\u001b[0m\u001b[1;32m   8111\u001b[0m             \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8112\u001b[0m         )\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   8133\u001b[0m                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8134\u001b[0m                 )\n\u001b[0;32m-> 8135\u001b[0;31m             return merge(\n\u001b[0m\u001b[1;32m   8136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8137\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mrindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         result_data = concatenate_block_managers(\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             b = make_block(\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0m_concatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     to_concat = [\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     to_concat = [\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     ]\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \u001b[0;31m# Check for EA to catch DatetimeArray, TimedeltaArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/arrays/sparse/array.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/competitive-data-science-predict-future-sales/venv/lib/python3.8/site-packages/pandas/core/arrays/sparse/array.py\u001b[0m in \u001b[0;36m_take_with_fill\u001b[0;34m(self, indices, fill_value)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# 1.) we took for an index of -1 (new)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;31m# 2.) we took a value that was self.fill_value (old)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0msp_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mnew_fill_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mold_fill_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msp_indexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mnew_fill_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vect_shop_cols = [col for col in shops.columns if col.startswith('vect')]\n",
    "all_data = all_data.join(shops[vect_shop_cols], on='shop_id', how='left')\n",
    "\n",
    "vect_item_cols = [col for col in items.columns if col.startswith('vect')]\n",
    "all_data = all_data.join(items[vect_item_cols], on='item_id', how='left')\n",
    "\n",
    "vect_item_cat_cols = [col for col in item_cats.columns if col.startswith('vect')]\n",
    "all_data = all_data.join(item_cats[vect_item_cat_cols], on='item_category_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c07e3",
   "metadata": {},
   "source": [
    "# Display the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f120575",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
